{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 6 and 7: \n",
    "# Operationalizing a Machine Learning Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Topics\n",
    " * Building Data Processing Pipelines\n",
    " * Implementing the Machine Learning Workflow\n",
    "\n",
    "![AppliedML_Workflow IMAGE](./images/AppliedML_Workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    " * Applied ML course Modules 1-5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labs &amp; Practices\n",
    "\n",
    "There are no labs in this module. However, the labs and practices in Module 1-5 will be good to review.  \n",
    "**More specifically, _Pipeline_, _Grid Search_, and _Class-Imbalance-Problem_ lessons.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "In this project, we will apply what we have learned in M1-M5. More specifically, we will be solving the **problem of predicting product backorder with an extremely imbalanced dataset**, where the minority class is less than 1%. Please see the preprocessing notebook (Part I) to learn about the dataset.\n",
    "\n",
    "While developing a solution, we apply data curation, anomaly detection, feature selection, and classification methods. We use the pipeline with grid-search to tune the parameters. This project is divided into three subtasks: a) data curation, b) model development, c) unbiased evaluation. Note that this is not the only way to solve this problem, but here we aim to apply what we have learned so far to implement an end-to-end ML process. \n",
    "\n",
    "For this project, we are given two datasets: training and test. We assume that unbiased model evaluation will be performed by someone else in Part 3, so we don't have access to the test set in Part 1 and Part 2. We have access to the training set in all three parts. The workflow is described in the following figure. \n",
    "\n",
    "\n",
    "![img](./images/model-development-framework-3.png)\n",
    "\n",
    "\n",
    "#### Part I \n",
    "\n",
    "In this part, we perform the following tasks: \n",
    "1. Drop columns that are obviously irrelevant. Don't drop too many columns as we will be performing feature selection in Part II. Hopefully, feature selection will take care of those irrelevant columns.\n",
    "1. Impute or remove null values. If you find that a few rows have null values, you can simply delete those rows. \n",
    "1. Encode colmuns with string values\n",
    "1. Create a balanced set for training models in Part II. See M5 to learn about smart sampling techniques with imbalanced datasets. Each account has roughly 4 GB of memory on Jupyterhub, so all of the sampling techniques may not be deployable on our system.  \n",
    "\n",
    "#### Part 2 \n",
    "\n",
    "1. Given the balanced training set, we will develop three **unique** pipelines. **By unique, we mean that a particular outlier detection method or a feature selection method, or a classifier can be used only once in all of these pipelines.** A pool of algorithms is provided in Part 2. \n",
    "\n",
    "1. First, we split the given balanced dataset into two datasets:  train-validation set and test set. We use the train-validation set for fitting the pipeline and fine-tuning the parameters.\n",
    "\n",
    "1. Each pipeline has three parts: a) outlier detection, b) feature selection and c) classification. It is possible to create a pipe with these three methods, but we need to write some custom codes if we want to add an outlier detection method inside the pipeline. For simplicity, we can do the following: \n",
    "\n",
    "    * We implement an outlier method independent of the pipeline (See M4 to learn about outlier detection methods)\n",
    "    * We then develop a pipeline with feature selection and classification and perform cross-validation and tune parameters. We use the output from the outlier method for fitting this pipeline (see M5, M1-M3 to learn about pipeline development, feature selection, and classification)\n",
    "\n",
    "1. We repeat Step 3 for creating three unique pipelines\n",
    "1. We identify the best pipeline and apply it to the test set to give an unbiased evaluation. Here we are mimicking the real-world scenario as we don't know who will be using our model in the future. So it is better to give an unbiased evaluation. \n",
    "1. We compare these pipelines and store the best pipeline. \n",
    "\n",
    "#### Part 3: \n",
    "\n",
    "1. In part 2, we identify the best pipeline and related hyperparameters. In part 3, we are given a new test set for evaluation. We can take advantage of the entire balanced training set that we used in Part 2. While keeping the estimated best hyperparameters fixed, we can retrain the best pipeline using the entire balanced set. Note that we are not required to perform a grid search here for parameter tuning. \n",
    "\n",
    "1. We then evaluate this retrained pipeline using the given test set (unbiased evaluation) and discuss the results. \n",
    "\n",
    "\n",
    "### Notebooks\n",
    "\n",
    "* [Part I - Preprocessing](./exercises/FP-Part1-Preprocessing.ipynb)\n",
    "* [Part II - Model Development](./exercises/FP-Part2-Model-Development.ipynb)\n",
    "* [Part II - Evaluation](./exercises/FP-Part3-Evaluation.ipynb)\n",
    "\n",
    "\n",
    "**This final project covers both M6 and M7. So you get two weeks to work on this.** These three notebooks are kept inside the Module 6 folder. Technically, Part 3 belongs to Module 7, but we kept it here due to it's dependency on the first two. **So you will find Module 7 empty.** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting your work\n",
    "\n",
    "#### Steps:\n",
    "  1. Open Terminal in JupyterHub\n",
    "  1. Change into the course folder\n",
    "  1. Stage (Git Add) the module's practive and exercise work   \n",
    "  `git  add    module6/exercises`\n",
    "  1. Create your work snapshot (Git Commit)  \n",
    "  `git   commit   -m   \"Module 6 submission\"`\n",
    "  1. Upload the snapshot to the server (Git Push)  \n",
    "  `git   push`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you have completed the learning activities for this module!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
