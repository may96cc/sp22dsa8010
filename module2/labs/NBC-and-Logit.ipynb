{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, Validate $\\rightarrow$ Train, Test \n",
    "### Focus: Naive Bayes Classifier & Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "When constructing a model, data availability may become an issue. \n",
    "In order to avoid overfitting, it is necessary to withhold some portion of the data as a test set. \n",
    "However, overfitting *on the test set* may also occur without a secondary validation step. \n",
    "As such, `scikit` contains a number of methods for cross-validation of data.\n",
    "\n",
    "## References\n",
    "1. [Scikit documentation - GaussianNB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "\n",
    "## Setting up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from collections import OrderedDict\n",
    "\n",
    "# load dataset \n",
    "\n",
    "DATASET = '/dsa/data/all_datasets/wine-quality/winequality-red.csv'\n",
    "dataset = pd.read_csv(DATASET, sep=';').sample(frac = 1).reset_index(drop=True)\n",
    "X = dataset.iloc[:, [1,2,6,9,10]]\n",
    "y = dataset.quality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler().fit(X_train)  \n",
    "# fitting should always be performed with X_train (y_train is labeled; so scaling is not required)\n",
    "# we assume that we don't have access to X_test, y_test\n",
    "\n",
    "\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_classifier = DummyClassifier()\n",
    "nb_classifier = GaussianNB()\n",
    "regression_classifier = LogisticRegression(max_iter = 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "Though a manual CV workflow was described in [the cross-validation lab](./CrossValidation.ipynb), the automated `cross_val_score()` will work well enough for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automated CV step\n",
    "baseline_scores = cross_val_score(baseline_classifier, X_train, y_train, cv=5)\n",
    "nb_scores = cross_val_score(nb_classifier, X_train, y_train, cv=5)\n",
    "regression_scores = cross_val_score(regression_classifier, X_train, y_train, cv=5)\n",
    "print(\"baseline score \", baseline_scores) # TODO: visualization of CV process\n",
    "print(\"NB score :\", nb_scores)\n",
    "print(\"Logistic Regression score: \", regression_scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we are performing cross validation with the training set. These cross-validation values represent how well (with 1 being a perfect score) the model performed against a small, as-yet-untrained portion of the data for the classification task.\n",
    "\n",
    "## Training the new models\n",
    "\n",
    "Since the CV values for NBC and logistic regression are higher than the baseline, we can create a model using all the data in the training set and test against the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit new model\n",
    "baseline_classifier.fit(X_train, y_train)\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "regression_classifier.fit(X_train, y_train)\n",
    "\n",
    "# model.predict() returns class labels (integers)\n",
    "y_pred_baseline = baseline_classifier.predict(X_test)\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "y_pred_reg = regression_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "confusion_matrix(y_test, y_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score comparision between the three models. \n",
    "Score shows that bayes classifier is better that baseline. \n",
    "Again Regression is better in score than Bayes classifier. \n",
    "\n",
    "For comparision we used all the given features. But you can try out several amount of features to test the models. \n",
    "if we increase the number of features then accuracy will increase. but it will require more computation to converge. \n",
    "Again reduction of number of features will loose accuracy. We need to trade-off between computation power and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, f1_score, recall_score, accuracy_score\n",
    "from pandas import DataFrame\n",
    "from IPython.display import display \n",
    "\n",
    "data_dict = {\n",
    "    'Metrics': [\"Precision\", 'Recall', 'F1', 'Recall'],\n",
    "    'Baseline': [\n",
    "        precision_score(y_test, y_pred_baseline, average= 'weighted'),  # weighted precision over all the classes\n",
    "        recall_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        f1_score(y_test, y_pred_baseline, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_baseline)\n",
    "    ],\n",
    "    \"Naive Bayes\": [\n",
    "        precision_score(y_test, y_pred_nb, average= 'weighted'),\n",
    "        recall_score(y_test, y_pred_nb, average='weighted'),\n",
    "        f1_score(y_test, y_pred_nb, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_nb)\n",
    "    ],\n",
    "    \"Logistic Regression\": [\n",
    "        precision_score(y_test, y_pred_reg, average= 'weighted'),\n",
    "        recall_score(y_test, y_pred_reg, average='weighted'),\n",
    "        f1_score(y_test, y_pred_reg, average='weighted'),\n",
    "        accuracy_score(y_test, y_pred_reg)\n",
    "    ]\n",
    "}\n",
    "table = np.around(DataFrame(data_dict), 2)\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell may throw warning as for some of the class labels `division by 0` error occurred while averaging the precision scores over class labels. You can see the formula for calculating performance measures for multi-label classification problem [here](https://scikit-learn.org/stable/modules/model_evaluation.html#precision-recall-f-measure-metrics). Here is the screenshot: \n",
    "\n",
    "<img src=\"./multi-label-performance-measure.jpg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comment:**\n",
    "\n",
    "* The naive Bayes classifier and logistic regression perform equally well as there is no significant differences between these two in terms of these four measures. \n",
    "* Warning is thrown as for some of the classes precision is 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
